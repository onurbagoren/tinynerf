{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import matplotlib\n",
    "\n",
    "import nerf_utils\n",
    "import Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Radiance Fields (NeRF)\n",
    "Contributors: Onur Bagoren, Hendrik Dreger, Wuao Liu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Datasets\n",
    "### Load the chair dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir datasets/chair\n",
    "!mkdir datasets/lego"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chair Dataset\n",
    "This dataset is one of the datasets tested by the original NeRF paper.\n",
    "In the next cells, we will load and visualize some of the images from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the root directory of the dataset\n",
    "root_dir = f'{sys.path[0]}/datasets/chair'\n",
    "chairs = Datasets.ChairData(root_dir, mode='train')\n",
    "chairs_loader = torch.utils.data.DataLoader(dataset=chairs, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 25 images on a 5,5 grid\n",
    "fig, axs = plt.subplots(5, 5, figsize=(15, 15))\n",
    "for ii, (image, _, _) in enumerate(chairs_loader):\n",
    "    if ii == 25:\n",
    "        break\n",
    "    row = ii // 5\n",
    "    col = ii % 5\n",
    "    axs[row, col].imshow(image[0])\n",
    "    axs[row, col].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing NeRF\n",
    "The NeRF algorithm is a method that aims to minimize the rendering error between images taken of the same object, from multiple viewpoints.\n",
    "\n",
    "In order to do this, the NeRf algorithm represents the state of the object as a 5-dimensional state $\\mathbf{X}$, where $(x, y, z)$ represents the camera position in the world frame, and $(\\theta, \\phi)$ represent the orientation of the camera.\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{X} = \n",
    "\\begin{bmatrix}\n",
    "x \\\\\n",
    "y \\\\\n",
    "z \\\\\n",
    "\\theta \\\\\n",
    "\\phi\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "In particular, the input can be though of consisting input position $\\mathbf{x} = \\left(x, y, z\\right)$ and direction $\\mathbf{d} = \\left(\\theta, \\phi\\right)$ of the camera.\n",
    "\n",
    "The ouput of the model is the volume function $\\sigma_i(t)$ and color $c_i(t)$ of points along the ray that projects from the camera to the object. The volume function represents the probability of the ray terminating at the point $t$.\n",
    "\n",
    "### The model architecture\n",
    "The architecture of the model is a multi-layer perceptron (MLP) with a ReLU activation functions. At certain layers, an encoded version of the input position $\\mathbf{x}$ or input direction $\\mathbf{d}$ is concatenated to a layer. A representation of the architecture is shown below.\n",
    "\n",
    "![Nerf-arch](images/NERF_arch.png)\n",
    "\n",
    "### Positional Encoding\n",
    "The positional encoding is a method to represent the input as a high-frequency function. \n",
    "A high frequency function is necessary, as an image is a high-frequency function, with frequent changes of the color gradients across the pixels. If the input to the network is not a high-frequency function, the network will struggle to learn how to represent the rendered image as a high-frequency function as well, struggling to capture fine textures and minute patterns.\n",
    "\n",
    "The paper uses $\\sin(x)$ and $\\cos(x)$ for the basis functions for the positional encoding. The number of functions used for positional encoding dictates the frequency of the encoded input, such that the frequency is directly proportional to $N^2$, where $N$ is the number of functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A demo of the positional encoding function used to represent an image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray-casting and sampling of query points\n",
    "\n",
    "In order to render the scene, it is necessary to first represent how the camera projects the scene onto the image plane. In conventinal graphics methods, this is done by projecting the scene onto the image plane using a perspective projection. However, in the NeRF algorithm, the camera is not perspective, and the projection is done by projecting the scene onto the image plane using a ray-casting method.\n",
    "\n",
    "The code below is a visualization of the function that does this ray cast and query point sampling. After running the cell, two figures will appear:\n",
    "1. The ray-cast in the camera frame\n",
    "2. The The ray cast in the worlds frame\n",
    "\n",
    "In both figures, the image plane is shown as a blue plane, the rays that go from the image plane to the camera pinhole (also referred to as the ray origin) are shown as red lines. The casted ray is then shown as a composition of yellow and blue lines. The blue line indicates the part of the ray where the query points are sampled from, called the query vector. The blue line begins at the \"near threshold\" and ends at the \"far threshold\". The yellow line represents the part of the ray that is cast from the ray origin to the beginning of the query vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualize import visualize_ray_casting\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "# Enter values to change the visualization\n",
    "roll = 0\n",
    "pitch = -torch.pi/16\n",
    "yaw = -torch.pi/16\n",
    "rot = R.from_euler('xyz', [roll, pitch, yaw], degrees=False)\n",
    "rot_mat = torch.tensor(rot.as_matrix())\n",
    "trans = torch.tensor([1, 0, 0])\n",
    "c2w = torch.eye(4)\n",
    "c2w[:3,:3] = rot_mat\n",
    "c2w[:3,3] = trans\n",
    "\n",
    "near_threshold = 1\n",
    "far_threshold = 20\n",
    "num_samples = 10\n",
    "H, W = (10, 10)\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "visualize_ray_casting(H, W, near_threshold, far_threshold, num_samples, c2w, fig)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating query points / representing the ray as a function for the model\n",
    "The ray-casting method is used to generate query points for the model. The query points are generated by sampling the ray-casting method at a certain number of points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Volume Rendering\n",
    "As an output from our model, we are receiving a color and a volume density at each depth point along the ray.\n",
    "We want to use these values and compute the color for each pixel in the image.\n",
    "We are iterating through each depth value along the image and compute the accumulated transmittance (T) and opacity (alpha). We then multiply them with the output color for the current depth position and sum over all  values. This gives us the predicted color for each pixel. The computation for the volume rendering is given below:\n",
    "\\begin{align}\n",
    "\n",
    "\\hat{C}(r) &= \\sum_{i=1}^{N} T_i\\left(1 - \\exp\\left(-\\sigma_i \\delta_i\\right)\\right)\\mathbf{c}_i \\\\\n",
    "T_i &= \\exp\\left(-\\sum_{j=1}^{{i-1}}\\sigma_j\\delta_j\\right)\n",
    "\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics for the model\n",
    "#### PSNR\n",
    "PSNR is the metric that evaluates the quality of the output. It is defined as the ratio of the maximum possible value of the output to the mean squared error between the output and the ground truth.\n",
    "\n",
    "#### Structural Similarity Index (SSIM)\n",
    "SSIM is a metric that can be used to measure how similar two images are to eachother. We use the SSIM metric that the skimage package provides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changes compared to the original model\n",
    "In order to reduce the training time for our model, we applied several changes:\n",
    "- Lower resolution images: Our images are transformed to 100x100 resolution\n",
    "- Fewer samples per ray: We are using fewer samples per ray during training (32)\n",
    "- No hierarchical sampling: We are just sampling along each point on the ray (32 points)\n",
    "- Smaller MLP: We reduced the size of our model. Instead of an 8-layer MLP with 256 hidden units per layer, we have 4 linear layers each with 256 hidden units per layer and do not concatenate the inputs from the positional encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on the Chair Dataset\n",
    "\n",
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir_chair = f'{sys.path[0]}/datasets/chair'\n",
    "chairs_train = Datasets.ChairData(root_dir, mode='train')\n",
    "train_loader_chairs = torch.utils.data.DataLoader(dataset=chairs_train, batch_size=1, shuffle=True)\n",
    "\n",
    "chairs_test = Datasets.ChairData(root_dir, mode='test')\n",
    "test_loader_chairs = torch.utils.data.DataLoader(dataset=chairs_test, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nerf_utils import Nerf\n",
    "num_encoding_functions = 6\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "input_dim = 2 * 3 * num_encoding_functions + 3\n",
    "\n",
    "model = Nerf(input_dim)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up he optimizer, scheduler and other hyper parameters\n",
    "lr = 5e-4\n",
    "num_iterations = int(1e3)\n",
    "log_interval = 100\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "test_data = Datasets.ChairData(root_dir, mode='test')\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=1, shuffle=True)\n",
    "fixed_test = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nerf_utils import train_synthetic\n",
    "\n",
    "# Fix the seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "trained_model_chair, train_losses, test_losses, ssims, psnrs = train_synthetic(\n",
    "    train_dataloader=train_loader_chairs,\n",
    "    test_dataloader=test_loader_chairs,\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    num_iterations=num_iterations,\n",
    "    log_interval_iterations=log_interval,\n",
    "    plot_intervals=num_iterations // 10,\n",
    "    device=device,\n",
    "    fixed_test=fixed_test,    \n",
    ")\n",
    "\n",
    "# Display the training plots\n",
    "fig, axs = plt.subplots(1, 3, figsize=(23, 5))\n",
    "N = len(train_losses)\n",
    "x = np.linspace(0, num_iterations, N)\n",
    "axs[0].plot(x, train_losses, label = 'train')\n",
    "axs[0].set_title('Loss Curves', fontsize=14)\n",
    "axs[0].plot(x, test_losses, label = 'test')\n",
    "axs[0].set_xlabel('Iteration')\n",
    "axs[0].set_ylabel('Loss')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(x, psnrs)\n",
    "axs[1].set_title('PSNR Curves', fontsize=14)\n",
    "axs[1].set_xlabel('Iteration')\n",
    "axs[1].set_ylabel('PSNR')\n",
    "\n",
    "axs[2].plot(x, ssims)\n",
    "axs[2].set_title('SSIM Curves', fontsize=14)\n",
    "axs[2].set_xlabel('Iteration')\n",
    "axs[2].set_ylabel('SSIM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on the Lego Dataset\n",
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nerf_utils import Nerf\n",
    "num_encoding_functions = 6\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "input_dim = 2 * 3 * num_encoding_functions + 3\n",
    "\n",
    "model = Nerf(input_dim)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir_lego = f'{sys.path[0]}/datasets/lego'\n",
    "train_lego = Datasets.ChairData(root_dir_lego, mode='train')\n",
    "train_loader_lego = torch.utils.data.DataLoader(dataset=train_lego, batch_size=1, shuffle=True)\n",
    "\n",
    "test_lego = Datasets.ChairData(root_dir_lego, mode='test')\n",
    "test_loader_lego = torch.utils.data.DataLoader(dataset=test_lego, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up he optimizer, scheduler and other hyper parameters\n",
    "lr = 5e-4\n",
    "num_iterations = int(1e3)\n",
    "log_interval = 100\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "fixed_test = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nerf_utils import train_synthetic\n",
    "\n",
    "trained_model_lego, train_losses, test_losses, ssims, psnrs = train_synthetic(\n",
    "    train_dataloader=train_loader_lego,\n",
    "    test_dataloader=test_loader_lego,\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    num_iterations=num_iterations,\n",
    "    log_interval_iterations=log_interval,\n",
    "    plot_intervals=num_iterations // 10,\n",
    "    device=device,\n",
    "    fixed_test=fixed_test,    \n",
    ")\n",
    "\n",
    "# Display the training plots\n",
    "fig, axs = plt.subplots(1, 3, figsize=(23, 5))\n",
    "N = len(train_losses)\n",
    "x = np.linspace(0, num_iterations, N)\n",
    "axs[0].plot(x, train_losses, label='train')\n",
    "axs[0].set_title('Loss Curves', fontsize=14)\n",
    "axs[0].plot(x, test_losses, label='test')\n",
    "axs[0].set_xlabel('Iteration')\n",
    "axs[0].set_ylabel('Loss')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(x, psnrs)\n",
    "axs[1].set_title('PSNR Curves', fontsize=14)\n",
    "axs[1].set_xlabel('Iteration')\n",
    "axs[1].set_ylabel('PSNR')\n",
    "\n",
    "axs[2].plot(x, ssims)\n",
    "axs[2].set_title('SSIM Curves', fontsize=14)\n",
    "axs[2].set_xlabel('Iteration')\n",
    "axs[2].set_ylabel('SSIM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Perspectives from the Training Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chair Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nerf_utils import generate_network_input, render_volume_density\n",
    "\n",
    "chair_val = Datasets.ChairData(root_dir_chair, mode='train')\n",
    "val_loader_chair = torch.utils.data.DataLoader(dataset=chair_val, batch_size=1, shuffle=True)\n",
    "\n",
    "for ii in range(5):\n",
    "    val_img, val_pose, val_focal = next(iter(val_loader_chair))\n",
    "    val_img = val_img[0,...].to(device)\n",
    "    val_pose = val_pose[0,...].to(device)\n",
    "    val_focal = val_focal[0].to(device)\n",
    "\n",
    "    network_input = generate_network_input(val_img, val_focal, val_pose)\n",
    "    encoded_points, query_points, depth_values = network_input\n",
    "\n",
    "    model_output = trained_model_chair(encoded_points)\n",
    "\n",
    "    predicted_rgb = render_volume_density(model_output, query_points, depth_values)\n",
    "\n",
    "    plotting_rgb = predicted_rgb.cpu().detach().numpy()\n",
    "    plotting_gt = val_img.cpu().detach().numpy()\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axs[0].imshow(plotting_gt)\n",
    "    axs[0].set_title('Original Image')\n",
    "    axs[1].imshow(plotting_rgb)\n",
    "    axs[1].set_title('Predicted Image')\n",
    "    fig.suptitle(f'Train Image {ii+1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lego Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nerf_utils import generate_network_input, render_volume_density\n",
    "\n",
    "lego_val = Datasets.ChairData(root_dir_lego, mode='train')\n",
    "val_loader_lego = torch.utils.data.DataLoader(dataset=lego_val, batch_size=1, shuffle=True)\n",
    "\n",
    "for ii in range(5):\n",
    "    val_img, val_pose, val_focal = next(iter(val_loader_lego))\n",
    "    val_img = val_img[0,...].to(device)\n",
    "    val_pose = val_pose[0,...].to(device)\n",
    "    val_focal = val_focal[0].to(device)\n",
    "\n",
    "    network_input = generate_network_input(val_img, val_focal, val_pose)\n",
    "    encoded_points, query_points, depth_values = network_input\n",
    "\n",
    "    model_output = trained_model_lego(encoded_points)\n",
    "\n",
    "    predicted_rgb = render_volume_density(model_output, query_points, depth_values)\n",
    "\n",
    "    plotting_rgb = predicted_rgb.cpu().detach().numpy()\n",
    "    plotting_gt = val_img.cpu().detach().numpy()\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axs[0].imshow(plotting_gt)\n",
    "    axs[0].set_title('Original Image')\n",
    "    axs[1].imshow(plotting_rgb)\n",
    "    axs[1].set_title('Predicted Image')\n",
    "    fig.suptitle(f'Train Image {ii+1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Perspectives Validation Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chair Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nerf_utils import generate_network_input, render_volume_density\n",
    "\n",
    "chair_val = Datasets.ChairData(root_dir_chair, mode='val')\n",
    "val_loader_chair = torch.utils.data.DataLoader(dataset=chair_val, batch_size=1, shuffle=True)\n",
    "\n",
    "for ii in range(5):\n",
    "    val_img, val_pose, val_focal = next(iter(val_loader_chair))\n",
    "    val_img = val_img[0,...].to(device)\n",
    "    val_pose = val_pose[0,...].to(device)\n",
    "    val_focal = val_focal[0].to(device)\n",
    "\n",
    "    network_input = generate_network_input(val_img, val_focal, val_pose)\n",
    "    encoded_points, query_points, depth_values = network_input\n",
    "\n",
    "    model_output = trained_model_chair(encoded_points)\n",
    "\n",
    "    predicted_rgb = render_volume_density(model_output, query_points, depth_values)\n",
    "\n",
    "    plotting_rgb = predicted_rgb.cpu().detach().numpy()\n",
    "    plotting_gt = val_img.cpu().detach().numpy()\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axs[0].imshow(plotting_gt)\n",
    "    axs[0].set_title('Original Image')\n",
    "    axs[1].imshow(plotting_rgb)\n",
    "    axs[1].set_title('Predicted Image')\n",
    "    fig.suptitle(f'Validation Image {ii+1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lego Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nerf_utils import generate_network_input, render_volume_density\n",
    "\n",
    "lego_val = Datasets.ChairData(root_dir_lego, mode='val')\n",
    "val_loader_lego = torch.utils.data.DataLoader(dataset=lego_val, batch_size=1, shuffle=True)\n",
    "for ii in range(5):\n",
    "    val_img, val_pose, val_focal = next(iter(val_loader_lego))\n",
    "    val_img = val_img[0,...].to(device)\n",
    "    val_pose = val_pose[0,...].to(device)\n",
    "    val_focal = val_focal[0].to(device)\n",
    "\n",
    "    network_input = generate_network_input(val_img, val_focal, val_pose)\n",
    "    encoded_points, query_points, depth_values = network_input\n",
    "\n",
    "    model_output = trained_model_lego(encoded_points)\n",
    "\n",
    "    predicted_rgb = render_volume_density(model_output, query_points, depth_values)\n",
    "\n",
    "    plotting_rgb = predicted_rgb.cpu().detach().numpy()\n",
    "    plotting_gt = val_img.cpu().detach().numpy()\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axs[0].imshow(plotting_gt)\n",
    "    axs[0].set_title('Original Image')\n",
    "    axs[1].imshow(plotting_rgb)\n",
    "    axs[1].set_title('Predicted Image')\n",
    "    fig.suptitle(f'Validation Image {ii+1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both the chair and the lego dataset, we are able to render images from novel perspectives. The loss curve is decreasing, the PSNR increasing and the SSIM is converging towards 1.\n",
    "Due to time constraints we were unable to run the model in the notebook for a long time, but we were able to run on the chair dataset for 5000 iterations. The output can be seen in the video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video('images/rendering_predicted_result.mp4', width=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Outputs from models we trained for longer\n",
    "\n",
    "#### 5000 Iterations on chair dataset, with metrics\n",
    "\n",
    "![](images/4800.png)\n",
    "![](images/Figure_1.png)\n",
    "\n",
    "#### 15000 Iterations on lego dataset\n",
    "\n",
    "![](images/14250.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c2f45844e7008c4b213ab29cc33ddbfaf8438e7e3bb3dcc65e398788ab3eeb36"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
